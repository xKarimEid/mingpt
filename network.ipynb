{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gpt(\n",
       "  (token_emb): Embedding(50257, 768)\n",
       "  (position_emb): Embedding(1024, 768)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mh_a): MHA(\n",
       "        (kqv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FFWD(\n",
       "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mh_a): MHA(\n",
       "        (kqv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FFWD(\n",
       "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mh_a): MHA(\n",
       "        (kqv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FFWD(\n",
       "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mh_a): MHA(\n",
       "        (kqv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FFWD(\n",
       "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mh_a): MHA(\n",
       "        (kqv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FFWD(\n",
       "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mh_a): MHA(\n",
       "        (kqv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FFWD(\n",
       "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mh_a): MHA(\n",
       "        (kqv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FFWD(\n",
       "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mh_a): MHA(\n",
       "        (kqv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FFWD(\n",
       "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mh_a): MHA(\n",
       "        (kqv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FFWD(\n",
       "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mh_a): MHA(\n",
       "        (kqv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FFWD(\n",
       "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mh_a): MHA(\n",
       "        (kqv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FFWD(\n",
       "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mh_a): MHA(\n",
       "        (kqv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffwd): FFWD(\n",
       "        (l1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (l2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config\n",
    "BATCH = 4\n",
    "VOCAB_SIZE = 50257\n",
    "N_EMBED = 768 \n",
    "CONTEXT_SIZE = 1024\n",
    "N_LAYERS = 12\n",
    "HEAD_SIZE = 64\n",
    "N_HEADS = int(N_EMBED // HEAD_SIZE)\n",
    "\n",
    "#-------------------------\n",
    "\n",
    "class FFWD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(N_EMBED, N_EMBED*4)\n",
    "        self.l2 = nn.Linear(N_EMBED*4, N_EMBED)\n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.l2(self.act(self.l1(x)))\n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.kqv = nn.Linear(N_EMBED, HEAD_SIZE*3*N_HEADS)\n",
    "        self.tril = torch.tril(torch.ones((CONTEXT_SIZE, CONTEXT_SIZE)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k, q, v = torch.split(self.kqv(x), HEAD_SIZE*N_HEADS, -1)\n",
    "        \n",
    "        k = k.view(B, T, N_HEADS, HEAD_SIZE).transpose(1, 2)\n",
    "        q = q.view(B, T, N_HEADS, HEAD_SIZE).transpose(1, 2)\n",
    "        v = v.view(B, T, N_HEADS, HEAD_SIZE).transpose(1, 2)\n",
    "\n",
    "        wei = k @ q.transpose(-1, -2) * HEAD_SIZE**-0.5\n",
    "        wei = torch.masked_fill(wei, self.tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim = -1)\n",
    "    \n",
    "        out = wei @ v\n",
    "        out = out.view(BATCH, CONTEXT_SIZE, N_EMBED)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(N_EMBED)\n",
    "        self.mh_a = MHA()\n",
    "        self.ln2 = nn.LayerNorm(N_EMBED)\n",
    "        self.ffwd = FFWD()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x)\n",
    "        x = self.mh_a(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.ffwd(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class gpt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(VOCAB_SIZE, N_EMBED)\n",
    "        self.position_emb = nn.Embedding(CONTEXT_SIZE, N_EMBED)\n",
    "        self.blocks = nn.Sequential(*\n",
    "            [Block() for _ in range(N_LAYERS)])\n",
    "        self.ffwd = FFWD()\n",
    "    \n",
    "    def forward(self, x, targets = None):\n",
    "        x = self.token_emb(x) + self.position_emb(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ffwd(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1024, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((BATCH, CONTEXT_SIZE, N_EMBED))\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "blocks = nn.Sequential(*[Block() for _ in range(N_LAYERS)])\n",
    "blocks(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for operation 1: 0.049138 seconds\n",
      "Time for operation 2: 24.167030 seconds\n"
     ]
    }
   ],
   "source": [
    "# Add this to a python notebook showing the difference between the two methods\n",
    "\n",
    "import timeit\n",
    "\n",
    "# Define the first operation\n",
    "def operation_1():\n",
    "    return q.view(BATCH, CONTEXT_SIZE, N_HEADS, HEAD_SIZE).transpose(1, 2)\n",
    "\n",
    "# Define the second operation\n",
    "def operation_2():\n",
    "    return q.reshape(BATCH, N_HEADS, CONTEXT_SIZE, HEAD_SIZE)\n",
    "\n",
    "# Time the first operation\n",
    "time_1 = timeit.timeit(operation_1, number=10000)\n",
    "\n",
    "# Time the second operation\n",
    "time_2 = timeit.timeit(operation_2, number=10000)\n",
    "\n",
    "print(f\"Time for operation 1: {time_1:.6f} seconds\")\n",
    "print(f\"Time for operation 2: {time_2:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
