{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
      "        [ 5120,   597,  2252,    11,  3285,   502],\n",
      "        [ 2740,    13,   198,   198,  3237,    25],\n",
      "        [  198,  5248,   461,    11,  2740,    13]])\n",
      "tensor([[22307,    25,   198,  8421,   356,  5120],\n",
      "        [  597,  2252,    11,  3285,   502,  2740],\n",
      "        [   13,   198,   198,  3237,    25,   198],\n",
      "        [ 5248,   461,    11,  2740,    13,   198]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch \n",
    "import tiktoken\n",
    "\n",
    "from gpt_network import GPT, GPTConfig\n",
    "\n",
    "\n",
    "data_path = os.path.join(os.path.dirname('__file__'), 'input.txt')\n",
    "with open(data_path, 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "data = data[:1000]\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "idx = enc.encode(data)\n",
    "idx = torch.tensor(idx)\n",
    "\n",
    "B, T = 4, 6\n",
    "d = idx[:B*T +1]\n",
    "\n",
    "x = d[:-1].view(B, T)\n",
    "print(x)\n",
    "\n",
    "l = d[1:].view(B, T)\n",
    "print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement data loader\n",
    "class LightDataLoader:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_batch(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.0682, grad_fn=<NllLossBackward0>)\n",
      "step 0 with loss: 11.068182945251465\n",
      "step 1 with loss: 4.436122417449951\n",
      "step 2 with loss: 1.2919187545776367\n",
      "step 3 with loss: 0.42711925506591797\n",
      "step 4 with loss: 0.1701803207397461\n",
      "step 5 with loss: 0.08746606111526489\n",
      "step 6 with loss: 0.05315355956554413\n",
      "step 7 with loss: 0.036159589886665344\n",
      "step 8 with loss: 0.027109066024422646\n",
      "step 9 with loss: 0.02193983644247055\n",
      "step 10 with loss: 0.018754970282316208\n",
      "step 11 with loss: 0.01662108488380909\n",
      "step 12 with loss: 0.015038863755762577\n",
      "step 13 with loss: 0.01373386476188898\n",
      "step 14 with loss: 0.01256487425416708\n",
      "step 15 with loss: 0.011483651585876942\n",
      "step 16 with loss: 0.010491251945495605\n",
      "step 17 with loss: 0.009601046331226826\n",
      "step 18 with loss: 0.00881969090551138\n",
      "step 19 with loss: 0.008142958395183086\n",
      "step 20 with loss: 0.007560847792774439\n",
      "step 21 with loss: 0.007059743162244558\n",
      "step 22 with loss: 0.006626916583627462\n",
      "step 23 with loss: 0.00625193677842617\n",
      "step 24 with loss: 0.005922373849898577\n",
      "step 25 with loss: 0.005629044026136398\n",
      "step 26 with loss: 0.00536440871655941\n",
      "step 27 with loss: 0.0051217107102274895\n",
      "step 28 with loss: 0.00489644892513752\n",
      "step 29 with loss: 0.004686012398451567\n",
      "step 30 with loss: 0.004487940110266209\n",
      "step 31 with loss: 0.004302531946450472\n",
      "step 32 with loss: 0.004128717351704836\n",
      "step 33 with loss: 0.003967863041907549\n",
      "step 34 with loss: 0.0038188963662832975\n",
      "step 35 with loss: 0.0036814280319958925\n",
      "step 36 with loss: 0.0035551816690713167\n",
      "step 37 with loss: 0.003439471125602722\n",
      "step 38 with loss: 0.003332964377477765\n",
      "step 39 with loss: 0.00323543231934309\n",
      "step 40 with loss: 0.003145412774756551\n",
      "step 41 with loss: 0.0030623648781329393\n",
      "step 42 with loss: 0.0029854688327759504\n",
      "step 43 with loss: 0.002913793781772256\n",
      "step 44 with loss: 0.002846857300028205\n",
      "step 45 with loss: 0.002784135052934289\n",
      "step 46 with loss: 0.0027251718565821648\n",
      "step 47 with loss: 0.002669162815436721\n",
      "step 48 with loss: 0.002616145880892873\n",
      "step 49 with loss: 0.0025656279176473618\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GPT(GPTConfig())\n",
    "\n",
    "logits, loss = model(x, l)\n",
    "print(loss)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr = 3e-4 )\n",
    "for i in range(50):\n",
    "    logits, loss = model(x, l)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(f\"step {i} with loss: {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
