{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "from prepare_data import DataLoader\n",
    "from gpt_network import GPT, GPTConfig\n",
    "from generate import generate_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n",
      "Hello, I'm a language model, Recording Abeエ variablesasionallyagogagogswitch Wid watered Buchanan amusing illegally retreated cheesy surv advancesantic CouDeliveryDate signaled restored\n",
      "Hello, I'm a language model, discrepancies Rulelv restored contin comparisonDes proposition specializing DDR delve delve exceptional too ASIC476 execution geo476 Switzerland MittML\n",
      "Hello, I'm a language model,エ sixth too Grad irradHur Riv resists exceptionalaugh disgustaugh advances restored476 ASIC irrad worksTC automatedML breach\n",
      "Hello, I'm a language model, sixth Odin951 too sixthuish Webb Music irrad mainland SCPgod BoosterohswitchNEWSichi breach678 colonizationtar lenses\n",
      "Hello, I'm a language model, actresses Hansen Hansencers Keep irrad variablesantic Xperia Premiership tolerateift disgust sacrificift Xperia ap PI Namespeakfuel593\n",
      "None\n",
      "Number of tokens: 338025\n",
      "Number of epochs: 2640\n",
      "step 0 with loss: 10.987454414367676\n",
      "step 1 with loss: 9.790398597717285\n",
      "step 2 with loss: 9.056686401367188\n",
      "step 3 with loss: 9.16458797454834\n",
      "step 4 with loss: 8.830345153808594\n",
      "step 5 with loss: 8.408177375793457\n",
      "step 6 with loss: 8.920641899108887\n",
      "step 7 with loss: 8.8939847946167\n",
      "step 8 with loss: 8.124495506286621\n",
      "step 9 with loss: 8.048580169677734\n",
      "step 10 with loss: 8.397934913635254\n",
      "step 11 with loss: 7.491004467010498\n",
      "step 12 with loss: 7.813939094543457\n",
      "step 13 with loss: 7.444109916687012\n",
      "step 14 with loss: 7.5397443771362305\n",
      "step 15 with loss: 7.405228614807129\n",
      "step 16 with loss: 7.487142562866211\n",
      "step 17 with loss: 8.28963851928711\n",
      "step 18 with loss: 7.199470043182373\n",
      "step 19 with loss: 7.8679656982421875\n",
      "step 20 with loss: 7.485372066497803\n",
      "step 21 with loss: 7.815303325653076\n",
      "step 22 with loss: 6.453542232513428\n",
      "step 23 with loss: 6.881099224090576\n",
      "step 24 with loss: 6.829766750335693\n",
      "step 25 with loss: 6.707175254821777\n",
      "step 26 with loss: 6.819349765777588\n",
      "step 27 with loss: 7.616204261779785\n",
      "step 28 with loss: 7.189713478088379\n",
      "step 29 with loss: 6.958916664123535\n",
      "step 30 with loss: 6.995051860809326\n",
      "step 31 with loss: 7.2275896072387695\n",
      "step 32 with loss: 7.135716915130615\n",
      "step 33 with loss: 7.008281230926514\n",
      "step 34 with loss: 7.884966850280762\n",
      "step 35 with loss: 7.7749810218811035\n",
      "step 36 with loss: 7.715291976928711\n",
      "step 37 with loss: 7.685444355010986\n",
      "step 38 with loss: 8.027729988098145\n",
      "step 39 with loss: 7.5454912185668945\n",
      "step 40 with loss: 7.416616916656494\n",
      "step 41 with loss: 6.990805149078369\n",
      "step 42 with loss: 7.133241176605225\n",
      "step 43 with loss: 7.129842281341553\n",
      "step 44 with loss: 7.000789165496826\n",
      "step 45 with loss: 7.115302085876465\n",
      "step 46 with loss: 6.271379470825195\n",
      "step 47 with loss: 6.418622016906738\n",
      "step 48 with loss: 6.977847099304199\n",
      "step 49 with loss: 6.84780216217041\n",
      "using device: cpu\n",
      "Hello, I'm a language model,\n",
      ",\n",
      " from;,, shall\n",
      "\n",
      ", howIA\n",
      "\n",
      " your:, himMAR.:\n",
      "Hello, I'm a language model, the.\n",
      "\n",
      ",.CI,:\n",
      " his\n",
      " it,\n",
      ": me.,'ll\n",
      "\n",
      "\n",
      "Hello, I'm a language model,.\n",
      "\n",
      "! with to have, ' to: the.\n",
      ".\n",
      " and your.: I.\n",
      "Hello, I'm a language model,N\n",
      "\n",
      " I'\n",
      "\n",
      ",\n",
      "\n",
      "., with,.:\n",
      " good,; it:\n",
      "Hello, I'm a language model,..'dius it\n",
      ", good\n",
      ": with his\n",
      " him;:-:::\n",
      " a\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Train configuration\n",
    "B, T = 4, 32\n",
    "\n",
    "# Detecting available device\n",
    "torch.manual_seed(1337)\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    torch.manual_seed(1337)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Creating model and moving to device\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "print(generate_tokens(model))\n",
    "\n",
    "train_data = DataLoader(B, T)\n",
    "\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr = 3e-4 )\n",
    "for i in range(50):\n",
    "    xb, yb = train_data.next_batch()\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(f\"step {i} with loss: {loss.item()}\")\n",
    "\n",
    "print(generate_tokens(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "\n",
    "layers = model.state_dict()\n",
    "layers.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 768])\n",
      "torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(layers['transformer.wte.weight'].shape)\n",
    "print(layers['lm_head.weight'].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
